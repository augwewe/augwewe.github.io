const n=JSON.parse(`{"key":"v-6499c3b0","path":"/Blog/2022/Crawl/foundation/Book1.html","title":"百度新闻爬取","lang":"zh-CN","frontmatter":{"title":"百度新闻爬取","date":"2022-10-30T12:04:21.000Z","author":"cava","isOriginal":true,"category":["notebook"],"tag":["爬虫技术","爬虫实战"],"icon":"vue","sticky":false,"star":false,"article":true,"timeline":true,"image":false,"navbar":true,"sidebarIcon":true,"headerDepth":5,"comment":true,"lastUpdated":true,"editLink":false,"backToTop":true,"toc":true,"description":"百度新闻网站爬取 流程图 import re,requests,tldextract,time def save_data(url,html): print(\\"%s :%s\\" %(url,len(html))) def crawler(): #1.下载百度页面 hub_url='https://news.baidu.com/' req=requests.get(hub_url) html=req.text # print(html) #2.获取新闻链接 # pattern='.*?&lt;link.*?href=\\"(.*?)\\".*?style=\\"zoom.*?&gt;.*?id=\\"ariaTipText\\".*?href=\\"\\"' pattern=r'href=[\\\\'\\"]?(.*?)[\\\\'\\"\\\\s]' news_links=re.findall(pattern,html) # print(news_links) #3.过滤链接 links_lst=[] for i in news_links: if not i.starswith('https'): continue tld=tldextract.extract(i) if tld.domain == 'baidu': continue links_lst.append(i) for k in links_lst: html=requests.get(k).text def main(): while 1: crawler() time.sleep() if __name__ == '__main__': crawler()","head":[["meta",{"property":"og:url","content":"https://augwewe.cn/Blog/2022/Crawl/foundation/Book1.html"}],["meta",{"property":"og:site_name","content":"augwewe"}],["meta",{"property":"og:title","content":"百度新闻爬取"}],["meta",{"property":"og:description","content":"百度新闻网站爬取 流程图 import re,requests,tldextract,time def save_data(url,html): print(\\"%s :%s\\" %(url,len(html))) def crawler(): #1.下载百度页面 hub_url='https://news.baidu.com/' req=requests.get(hub_url) html=req.text # print(html) #2.获取新闻链接 # pattern='.*?&lt;link.*?href=\\"(.*?)\\".*?style=\\"zoom.*?&gt;.*?id=\\"ariaTipText\\".*?href=\\"\\"' pattern=r'href=[\\\\'\\"]?(.*?)[\\\\'\\"\\\\s]' news_links=re.findall(pattern,html) # print(news_links) #3.过滤链接 links_lst=[] for i in news_links: if not i.starswith('https'): continue tld=tldextract.extract(i) if tld.domain == 'baidu': continue links_lst.append(i) for k in links_lst: html=requests.get(k).text def main(): while 1: crawler() time.sleep() if __name__ == '__main__': crawler()"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-02-18T04:42:15.000Z"}],["meta",{"property":"article:author","content":"cava"}],["meta",{"property":"article:tag","content":"爬虫技术"}],["meta",{"property":"article:tag","content":"爬虫实战"}],["meta",{"property":"article:published_time","content":"2022-10-30T12:04:21.000Z"}],["meta",{"property":"article:modified_time","content":"2023-02-18T04:42:15.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"百度新闻爬取\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2022-10-30T12:04:21.000Z\\",\\"dateModified\\":\\"2023-02-18T04:42:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"cava\\"}]}"]]},"headers":[{"level":2,"title":"百度新闻网站爬取","slug":"百度新闻网站爬取","link":"#百度新闻网站爬取","children":[{"level":3,"title":"流程图","slug":"流程图","link":"#流程图","children":[]},{"level":3,"title":"补充：tldextract","slug":"补充-tldextract","link":"#补充-tldextract","children":[]}]}],"git":{"createdTime":1676651936000,"updatedTime":1676695335000,"contributors":[{"name":"AndersonHJB","email":"aiyuechuang@gmail.com","commits":1},{"name":"augwewe","email":"1085816416@qq.com","commits":1}]},"readingTime":{"minutes":1.24,"words":371},"filePathRelative":"Blog/2022/Crawl/foundation/Book1.md","localizedDate":"2022年10月30日","excerpt":"<h2> 百度新闻网站爬取</h2>\\n<h3> 流程图</h3>\\n\\n<div class=\\"language-python line-numbers-mode\\" data-ext=\\"py\\"><pre class=\\"language-python\\"><code><span class=\\"token keyword\\">import</span> re<span class=\\"token punctuation\\">,</span>requests<span class=\\"token punctuation\\">,</span>tldextract<span class=\\"token punctuation\\">,</span>time\\n\\n<span class=\\"token keyword\\">def</span> <span class=\\"token function\\">save_data</span><span class=\\"token punctuation\\">(</span>url<span class=\\"token punctuation\\">,</span>html<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n    <span class=\\"token keyword\\">print</span><span class=\\"token punctuation\\">(</span><span class=\\"token string\\">\\"%s :%s\\"</span> <span class=\\"token operator\\">%</span><span class=\\"token punctuation\\">(</span>url<span class=\\"token punctuation\\">,</span><span class=\\"token builtin\\">len</span><span class=\\"token punctuation\\">(</span>html<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">)</span>\\n\\n<span class=\\"token keyword\\">def</span> <span class=\\"token function\\">crawler</span><span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n    <span class=\\"token comment\\">#1.下载百度页面</span>\\n    hub_url<span class=\\"token operator\\">=</span><span class=\\"token string\\">'https://news.baidu.com/'</span>\\n    req<span class=\\"token operator\\">=</span>requests<span class=\\"token punctuation\\">.</span>get<span class=\\"token punctuation\\">(</span>hub_url<span class=\\"token punctuation\\">)</span>\\n    html<span class=\\"token operator\\">=</span>req<span class=\\"token punctuation\\">.</span>text\\n    <span class=\\"token comment\\"># print(html)</span>\\n\\n    <span class=\\"token comment\\">#2.获取新闻链接</span>\\n    <span class=\\"token comment\\"># pattern='.*?&lt;link.*?href=\\"(.*?)\\".*?style=\\"zoom.*?&gt;.*?id=\\"ariaTipText\\".*?href=\\"\\"'</span>\\n    pattern<span class=\\"token operator\\">=</span><span class=\\"token string\\">r'href=[\\\\'\\"]?(.*?)[\\\\'\\"\\\\s]'</span>\\n    news_links<span class=\\"token operator\\">=</span>re<span class=\\"token punctuation\\">.</span>findall<span class=\\"token punctuation\\">(</span>pattern<span class=\\"token punctuation\\">,</span>html<span class=\\"token punctuation\\">)</span>\\n    <span class=\\"token comment\\"># print(news_links)</span>\\n\\n    <span class=\\"token comment\\">#3.过滤链接</span>\\n    links_lst<span class=\\"token operator\\">=</span><span class=\\"token punctuation\\">[</span><span class=\\"token punctuation\\">]</span>\\n    <span class=\\"token keyword\\">for</span> i <span class=\\"token keyword\\">in</span> news_links<span class=\\"token punctuation\\">:</span>\\n        <span class=\\"token keyword\\">if</span> <span class=\\"token keyword\\">not</span> i<span class=\\"token punctuation\\">.</span>starswith<span class=\\"token punctuation\\">(</span><span class=\\"token string\\">'https'</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n            <span class=\\"token keyword\\">continue</span>\\n        tld<span class=\\"token operator\\">=</span>tldextract<span class=\\"token punctuation\\">.</span>extract<span class=\\"token punctuation\\">(</span>i<span class=\\"token punctuation\\">)</span>\\n        <span class=\\"token keyword\\">if</span> tld<span class=\\"token punctuation\\">.</span>domain <span class=\\"token operator\\">==</span> <span class=\\"token string\\">'baidu'</span><span class=\\"token punctuation\\">:</span>\\n            <span class=\\"token keyword\\">continue</span>\\n        links_lst<span class=\\"token punctuation\\">.</span>append<span class=\\"token punctuation\\">(</span>i<span class=\\"token punctuation\\">)</span>\\n    <span class=\\"token keyword\\">for</span> k <span class=\\"token keyword\\">in</span> links_lst<span class=\\"token punctuation\\">:</span>\\n        html<span class=\\"token operator\\">=</span>requests<span class=\\"token punctuation\\">.</span>get<span class=\\"token punctuation\\">(</span>k<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">.</span>text\\n\\n<span class=\\"token keyword\\">def</span> <span class=\\"token function\\">main</span><span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n    <span class=\\"token keyword\\">while</span> <span class=\\"token number\\">1</span><span class=\\"token punctuation\\">:</span>\\n        crawler<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n        time<span class=\\"token punctuation\\">.</span>sleep<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n<span class=\\"token keyword\\">if</span> __name__ <span class=\\"token operator\\">==</span> <span class=\\"token string\\">'__main__'</span><span class=\\"token punctuation\\">:</span>\\n    crawler<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n</code></pre><div class=\\"line-numbers\\" aria-hidden=\\"true\\"><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div></div></div>","autoDesc":true}`);export{n as data};
